{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 - a single good with fixed odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Basic Setup__:\n",
    "\n",
    "First things first, we consider the simplest situation where a cafe sells only one product (say X). We further assume that every customer always uses the app if it's available. \n",
    "\n",
    "- $p = \\text{price of one X} = 3.5\\$$\n",
    "- $r = \\text{raw cost of one X} = 0.8\\$ $\n",
    "\n",
    "(Source: http://www.cafe-coach.com.au/the-secret-to-real-profits-in-a-cup-of-coffee/)\n",
    "\n",
    "Every time a customer purchases a cup of coffee, our app offers a voucher with two possible discount rates which occur with different probabilities. To formalise, we define the variables below\n",
    "\n",
    "- $N = \\text{average number of customers per day}$\n",
    "- $N_{app} = \\text{average number of customers with the app per day}$\n",
    "- $d_W = \\text{discount rate of winning}$,  $q_W = \\text{probability of winning}$ \n",
    "- $d_L = \\text{discount rate of losing}$,   $q_L = \\text{probability of losing}$ \n",
    "\n",
    "For the time being, we assume that the discount rates and winning probability are fixed. Here, discount rate is defined as the ratio of the discounted price with respect to the original price $p$. Hence, if you win a gamble, you can purchase a good X for price $q_W\\cdot p$ and if you lose, you get it for $q_L\\cdot p$. So, for instance, if you use the app with parameters $d_W = 0.6, q_W = 0.2, d_L = 0.9, q_L = 0.8$, then you win 40% discount off the price $p$ with probability 20% but only 10% off with the remaining probability.    \n",
    "\n",
    "For the time being we assume that results of a gamble is binary (win or lose), and thus $q_W + q_L = 1$. Also, for the shop to make profits with the discounts, we need to make sure the discounted prices $d_{W}p$ and $d_{L}p$ are above the raw cost $r = 0.8\\$$. Also, you should expect better discount rates when you win than when you lose. So, in summary we have:\n",
    "\n",
    "$$ \\frac{r}{p}\\approx 0.22857 < d_W < d_L < 1.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Simulation 1.1__: Dependency of $R$ on $d_{W},d_{L}$ and $q_w$\n",
    "\n",
    "Given the chosen discount scheme (characterised by $d_{W},d_{L}$ and $q_w$), we investigate how much we need to increase the average number of customers to achieve a desired profit increase.\n",
    "\n",
    "The daily expected profit without the app is given by: $$DEP := N\\cdot (p-r).$$ \n",
    "\n",
    "Similarly, assuming every customer uses the app after its release, the daily expected profit with the app is given by: $$DEP_{app}:=N_{app}\\cdot \\Big{[}q_{W}\\cdot(d_{W}p - r) + (1-q_{W})\\cdot(d_{L}p-r)\\Big{]}.$$\n",
    "\n",
    "Now, say we aim to increase the profits of the shop by the factor of  $\\alpha > 1$; we want to ensure the inequality $$ \\alpha \\cdot DEP < DEP_{app}.$$ To guarantee this happens, we at least need to increase the average number of customers by the factor of: $$ R(\\alpha, d_{W},d_{L},q_{W},q_{L}) := \\frac{\\alpha(p-r)}{\\Big{[}q_{W}\\cdot(d_{W}p - r) + (1-q_{W})\\cdot(d_{L}p-r)\\Big{]}}$$\n",
    "\n",
    "Now, with the chosen discount scheme $(d_{W},d_{L},q_w)$, we want to know by how much we need to increase the number of customers (i.e the value of $R$) so as to achieve the target profit boost $\\alpha$. To this end, we look at the surface $R$ as a function of $d_{W},d_{L}$ and $q_w$. However, this is a 4 dimensional surface, and so we assign a fixed value to $q_w$ and only visualise the corresponding landscape of $R$ as a function $d_{W},d_{L}$ (see the figure below). You should try and see how the surface looks as you set different values of $q_w$ in the following code.  \n",
    "\n",
    "Note. We can similarly model the conventional voucher scheme (in which a fixed rate of discount is guaranteed) as the case where $d_{W} = d_{L}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are Working with Matplotlib in a virtual enviroment see 'Working with Matplotlib in Virtual environments' in the Matplotlib FAQ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d304efc82d10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmplot3d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/derekmok/Envs/VottoAlgoService/lib/python2.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab_setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0m_backend_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_figure_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_if_interactive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpylab_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0m_IP_REGISTERED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/derekmok/Envs/VottoAlgoService/lib/python2.7/site-packages/matplotlib/backends/__init__.pyc\u001b[0m in \u001b[0;36mpylab_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# imports. 0 means only perform absolute imports.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     backend_mod = __import__(backend_name,\n\u001b[0;32m---> 32\u001b[0;31m                              globals(),locals(),[backend_name],0)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Things we pull in from all backends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/derekmok/Envs/VottoAlgoService/lib/python2.7/site-packages/matplotlib/backends/backend_macosx.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_macosx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are Working with Matplotlib in a virtual enviroment see 'Working with Matplotlib in Virtual environments' in the Matplotlib FAQ"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "#fixed parameters:\n",
    "p = 3.5\n",
    "r = 0.8\n",
    "alpha = 1.1\n",
    "q_w = 0.9 #winning probability - try different values!\n",
    "\n",
    "#define R:\n",
    "def R(alpha,d_w, d_l,q_w):\n",
    "  return alpha*(p-r)/(q_w*(d_w*p-r)+(1-q_w)*(d_l*p-r))\n",
    "\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "Start = 0.5 #0.23\n",
    "End = 1.0#1.0 \n",
    "d_w = d_l = np.linspace(Start, End, 150)\n",
    "D_W, D_L = np.meshgrid(d_w, d_l)\n",
    "Rs = np.array([R(alpha,k1,k2,q_w) for k1,k2 in zip(np.ravel(D_W), np.ravel(D_L))])\n",
    "#print 'Rs.shape:',Rs.shape\n",
    "#print 'D_W.shape:', D_W.shape\n",
    "R = Rs.reshape(D_W.shape)\n",
    "\n",
    "r = alpha*(p-r)/(q_w*(d_w*p-r)+(1-q_w)*(d_l*p-r))\n",
    "#print 'size of r:', r.size\n",
    "\n",
    "ax.plot_surface(D_W, D_L, R)\n",
    "ax.plot(d_w,d_l,r, color = \"red\", linestyle = 'dashed', linewidth = 4)\n",
    "\n",
    "ax.set_xlabel('$d_W$')\n",
    "ax.set_ylabel('$d_L$')\n",
    "ax.set_zlabel('$R$')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we impose $d_w < d_l$, we are only interested in the upper half of the surface above the dotted line $d_w = d_l$. This provides a landscape of $R$ (minimal increase in the number of customers necessary to ensure profit increase $\\alpha$) as a function of $d_w$,$d_l$ and $q_w$. As the discount rates $d_l$ and $d_w$ get closer to the lower bound $r/p$, the value of $R$ tends to infinity as expected. On the other hand, as the winning discount rate $d_w$ gets closer to $1$ (i.e no discounts), the value of $R$ would be exactly $\\alpha$: with no discounts, we need to increase the number of customers by the factor of $\\alpha$ to achieve profit increase by the factor of $\\alpha$. Also notice that the winning probability $q_w$ adjusts the levels of dependecy of R on $d_w$ and $d_l$. For example, the winning probability is very high (say $q_w = 0.99$), the landscape of R is alsmost completely determined by $d_w$, indepent of the value of $d_l$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Simulation 1.2__:\n",
    "\n",
    "In this section, on the assumption that our app can increase the number of customers by $R>1$, we now look at the set of all possible discount options which can achieve the target profit increase $\\alpha$. \n",
    "\n",
    "This is equivalent to looking at the cross-section of the surface $R(\\alpha, d_{W},d_{L},q_{W},q_{L})$ sliced up at some values of $R$ and $\\alpha$. In other words, we are interested in the following set:\n",
    "\n",
    "$$ D_{\\text{options}}(\\alpha,R):= \\Big{\\{}(d_w,d_l,q_w) : \\, \\frac{N_{fam}}{N} = R,\\,\\,\\,\\, \\alpha \\cdot DEP = DEP_{app},\\,\\,\\,\\, \\frac{r}{p}< d_W < d_L < 1\\Big{\\}}.$$\n",
    "\n",
    "After some algebra, you can show that this set is given by: \n",
    "\n",
    "\\begin{align}\n",
    "D_{\\text{options}}(\\alpha,R)&= \\Big{\\{}(d_w,d_l,q_w) :\\, d_w = ad_l + b,\\,\\, q_w b < d_l < min(a^{-1}(r/p - b),1),\\,\\, 0<q_w < 1 \\Big{\\}}\\\\\n",
    "&=\\bigcup_{0<q_w < 1}\\Big{\\{}(d_w,d_l,q_w) :\\, d_w = ad_l + b,\\,\\, q_w b < d_l < min(a^{-1}(r/p - b),1) \\Big{\\}}\n",
    "\\end{align}\n",
    "where $a = (q_w-1)/q_w$ and $b=(\\alpha/R + r(1-\\alpha/R)/p)/q_w$. You can further simplify $b$, but did not bother.\n",
    "\n",
    "\n",
    "The following interactive plot allows you to look at the slice of $D_{\\text{options}}(\\alpha,R)$ for your chosen value of $q_w$ (select on the slider). -- HOW ABOUT PLOTTING A 3D SURFACE AS WELL???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "from IPython.html.widgets import interact\n",
    "import math\n",
    "\n",
    "def discount_rate(R, q_w):\n",
    "    p = 3.5\n",
    "    r = 0.8\n",
    "    alpha = 1.1\n",
    "    grad_l = -(1-q_w)/q_w\n",
    "    intcept_l = (alpha/R + r/p*(1-alpha/R))/q_w\n",
    "    Start = max(r/p, q_w*intcept_l)\n",
    "    End = min(1,(r/p - intcept_l)/grad_l)\n",
    "    d_l = np.linspace(Start, End, 200)\n",
    "    d_w = grad_l*d_l + intcept_l\n",
    "    plt.plot(d_l,d_w,'g',linewidth =3)\n",
    "    nogamble = q_w*intcept_l\n",
    "    plt.plot(nogamble,nogamble, marker='o',markerfacecolor='red',markersize = 10)\n",
    "    \n",
    "    plt.title(\"R = (%s), q_w = (%s), equivalent guaranteed discount = %s\" %(R, q_w, nogamble) )\n",
    "    plt.grid()\n",
    "    plt.xlabel('Lose discount rate, $d_l$')\n",
    "    plt.ylabel('Win discount rate, $d_w$')  \n",
    "    plt.ylim([0,1])\n",
    "    plt.xlim([0,1])\n",
    "    \n",
    "\n",
    "interact(discount_rate, R=(1.1,2.0,0.05),q_w=(0.0,1.0,0.05)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the green line shows the cross-section of $D_{\\text{options}}(\\alpha,R)$ at a chosen value of winning probability $q_w$. As long as you are on this surface, the profit increase $\\alpha$ is guaranteed in expextation assuming the number of customers increase by the factor of $R$. The red marker shows the point where $d_l = d_w$. \n",
    "\n",
    "For the given value of $R$, the smaller the winning probability, the more risky the gamble can be made i.e. the larger the difference in discount rates between 'win' and 'lose' (so, if you win, you save a lot).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "from IPython.html.widgets import interact\n",
    "import math\n",
    "\n",
    "\n",
    "def discount_dollars(R, q_w):\n",
    "    p = 3.5\n",
    "    r = 0.8\n",
    "    alpha = 1.10\n",
    "    grad_l = -(1-q_w)/q_w\n",
    "    intcept_l = (alpha/R + r/p*(1-alpha/R))/q_w\n",
    "    Start = max(r/p, q_w*intcept_l)\n",
    "    End = min(1,(r/p - intcept_l)/grad_l)\n",
    "    d_l = np.linspace(Start, End, 200)\n",
    "    d_w = grad_l*d_l + intcept_l\n",
    "    d_l_dollar = p*(1-d_l)\n",
    "    d_w_dollar = p*(1-d_w)\n",
    "    plt.plot(d_l_dollar, d_w_dollar,color = \"blue\", linewidth = 3)\n",
    "    nogamble_dollar = p*(1 - q_w*intcept_l)\n",
    "    plt.plot(nogamble_dollar,nogamble_dollar, marker='o',markerfacecolor='red',markersize = 10)\n",
    "    \n",
    "    plt.title(\"R = (%s), q_w = (%s), equivalent guaranteed discount = %s$\" %(R, q_w, nogamble_dollar) )\n",
    "    plt.xlabel('Lose and save ($)')\n",
    "    plt.ylabel('Win and save ($)')\n",
    "    plt.ylim([0,p-r])\n",
    "    plt.xlim([0,p-r])\n",
    "    plt.grid()\n",
    "\n",
    "    #plt.axes([])    \n",
    "\n",
    "interact(discount_dollars, R=(1.1,2.0,0.05),q_w=(0.0,1.0,0.05)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 - a single good with adaptive odds.\n",
    "\n",
    "In the previous section, we saw that, given the number of customers increase by $R$, then the profit also increases by $\\alpha$ in expectation i.e. $E(DEP_{app}/DEP) = \\alpha $ for any discount schemes $(d_w, d_l, q_w) \\in D_{\\text{options}}(\\alpha,R)$. However, in practice, we want to improve the discount options on offer as customers win, to incentivise them to come back. On the other hand, we want to reduce the discount rates if customers lose. To do this, we will vary the target profit increase $\\alpha$. This is fine as long as the expected profit increase $E(DEP_{app}/DEP) = E(\\alpha)$ (note $\\alpha$ is a random variable, not a constant) is sufficiently above $1$ from the shop's point of view.\n",
    "\n",
    "Let us denote the discount scheme at time $t$ by $(d^{(t)}_w,d^{(t)}_l,q^{(t)}_w)$ and the result of the gamble by $outcome^{(t)}$. Could we come up a reasonable way to determine $(d^{(t+1)}_w,d^{(t+1)}_l,q^{(t+1)}_w)$ on the basis of history $H(t) := \\{(d^{(1)}_w,d^{(t)}_l,q^{(1)}_w),outcome^{(1)},...,(d^{(t)}_w,d^{(t)}_l,q^{(t)}_w),outcome^{(t)}\\}$? \n",
    "\n",
    "_Multi option scheme_: How about letting users choose from several discount options? e.g four options: 'true gambler', 'average gambler','safe player' and 'moral person' with winning probability fixed at $q_w = 25\\%$, $50\\%$, $75\\%$ and $100\\%$. For each, we display $(q_w, q_l)$ in the manner that the difference $d_l - d_w$ is increasingly larger for more risky option (i.e. smaller winning probability $q_w$). If you win or lose, you vary the objective $\\alpha$ in a range $(1,R)$ and change the discount rates in respective options accordingly. In the event of a win, decrease the alpha i.e. $\\alpha^{(t)} > \\alpha^{(t+1)}$ and otherwise decrease. The initial value is set to be $R/2$ and the width of increment (the amount by which $\\alpha$ changes) is a decreasing function of $q_w$ i.e. the more risky (the less likely to win), the larger change in the next discount options. Discount rates are so far only updated based on the outcome at the previous time step (Markov property), but we could lift this assumption. \n",
    "\n",
    "\n",
    "\n",
    "__Simulation 2.1__: 'Multiple option scheme'\n",
    "\n",
    "In this section, we will investigate the 'Multiple option scheme' discribed above. To continue ...\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Limitations of the current model__:\n",
    "- assumed every customer uses the app. In reality, some transaction are performed without the app. \n",
    "- assumed the shop sell only one product - the extension is very easy though; you just need to have separate systems. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "NOTES:\n",
    "Interactive plot with python:\n",
    "see http://moderndata.plot.ly/widgets-in-ipython-notebook-and-plotly/\n",
    "see https://www.youtube.com/watch?v=wxVx54ax47s\n",
    "see http://bokeh.pydata.org/en/latest/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
